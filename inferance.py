#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤šæ¨¡æ€æ¨ç†ä»£ç 
"""

import os
import torch
from PIL import Image
from utils import load_model, load_processor


def load_trained_model(checkpoint_path, device="cuda"):
    """
    åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
    
    Args:
        checkpoint_path: è®­ç»ƒåæ¨¡å‹çš„è·¯å¾„
        device: è¿è¡Œè®¾å¤‡
        
    Returns:
        model, processor
    """
    print(f"æ­£åœ¨åŠ è½½è®­ç»ƒåçš„æ¨¡å‹: {checkpoint_path}")
    
    # ä½¿ç”¨åŸå§‹çš„æ¨¡å‹æ„å»ºæ–¹å¼
    model = load_model(device)
    processor = load_processor()
    
    # åŠ è½½è®­ç»ƒåçš„æƒé‡
    if os.path.exists(os.path.join(checkpoint_path, "model.safetensors")):
        print("æ­£åœ¨åŠ è½½safetensorsæƒé‡...")
        from safetensors.torch import load_file
        state_dict = load_file(os.path.join(checkpoint_path, "model.safetensors"))
        model.load_state_dict(state_dict, strict=False)
        print("âœ… æƒé‡åŠ è½½æˆåŠŸ")
    elif os.path.exists(os.path.join(checkpoint_path, "pytorch_model.bin")):
        print("æ­£åœ¨åŠ è½½pytorchæƒé‡...")
        state_dict = torch.load(os.path.join(checkpoint_path, "pytorch_model.bin"), map_location=device)
        model.load_state_dict(state_dict, strict=False)
        print("âœ… æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
    
    model.eval()
    return model, processor


def inference(model, processor, image_path, prompt, max_tokens=512, device="cuda"):
    """
    ç®€å•çš„æ¨ç†å‡½æ•°
    
    Args:
        model: åŠ è½½çš„æ¨¡å‹
        processor: å¤„ç†å™¨
        image_path: å›¾åƒè·¯å¾„
        prompt: æ–‡æœ¬æç¤º
        max_tokens: æœ€å¤§tokenæ•°
        device: è®¾å¤‡
        
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    # åŠ è½½å›¾åƒ
    if isinstance(image_path, str):
        image = Image.open(image_path).convert('RGB')
    else:
        image = image_path
    
    messages = [
        {
            "role": "system",
            "content": "ä½¿ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚",
        },
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": prompt},
            ],
        },
    ]
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # å¤„ç†è¾“å…¥
    inputs = processor(text=text, images=image, return_tensors="pt")
    inputs = inputs.to(device)
    
    # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸æ¨¡å‹æƒé‡åŒ¹é…ï¼ˆbfloat16ï¼‰
    for key in inputs:
        if key == 'pixel_values' and inputs[key] is not None:
            inputs[key] = inputs[key].to(torch.bfloat16)
        elif key == 'input_ids' and inputs[key] is not None:
            inputs[key] = inputs[key].to(device)
        elif key == 'attention_mask' and inputs[key] is not None:
            inputs[key] = inputs[key].to(device)
    
    # ç”Ÿæˆå›å¤
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            use_cache=True
        )
    
    # è§£ç è¾“å‡º
    generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
    response = processor.decode(generated_ids, skip_special_tokens=True)
    
    return response.strip()


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    model_path = "./model/staged_training_test/stage2"
    image_path = "./resource/dog.png"  # æ¼”ç¤ºå›¾ç‰‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print("ğŸš€ å¼€å§‹æ¨ç†æ¼”ç¤º...")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è®¾å¤‡: {device}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    if not os.path.exists(image_path):
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        model, processor = load_trained_model(model_path, device)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æµ‹è¯•æ¨ç†
        prompts = [
            "è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚",
            "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆä¸œè¥¿ï¼Ÿ",
            "å›¾ä¸­çš„æ•°é‡æœ‰å¤šå°‘ï¼Ÿ"
        ]
        
        print(f"\nğŸ“¸ æµ‹è¯•å›¾ç‰‡: {image_path}")
        print("="*60)
        
        for i, prompt in enumerate(prompts, 1):
            print(f"\n{i}. æç¤º: {prompt}")
            print("-" * 50)
            
            try:
                response = inference(model, processor, image_path, prompt, device=device)
                print(f"å›å¤: {response}")
            except Exception as e:
                print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        
        print("\n" + "="*60)
        print("æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
